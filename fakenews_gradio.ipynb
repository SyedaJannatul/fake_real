{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "snODIgli90gq",
    "outputId": "e57f5ac3-3e7b-4d25-87d3-81035eee831f"
   },
   "outputs": [],
   "source": [
    "!pip install gradio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zjnM1CYMKbsJ",
    "outputId": "e9ea46ac-ebdd-4d55-dc18-682006bbf77f"
   },
   "outputs": [],
   "source": [
    "!pip install langdetect lime -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJl89bks3pOX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import io\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langdetect import detect\n",
    "import torch\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import gradio as gr\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIu9kanqCHdG"
   },
   "source": [
    "# Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67T3ewmIEP1G"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUnYW6i1GPEO"
   },
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"Jannat24/finetuned_mbert_fakenews_bn_en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmL16TXeGlIP"
   },
   "outputs": [],
   "source": [
    "txt = \"ছাত্র আন্দোলনের সময় ৩২০৪ জন পুলিশকে হত্যা করা হয়েছে- টাইমস ম্যাগাজিন।\"\n",
    "l = len(txt.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ana5CElgGk_8",
    "outputId": "43e7440c-e402-4361-cafa-373f7ffbdf80"
   },
   "outputs": [],
   "source": [
    "pipeline_model = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "outputs = pipeline_model(txt)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laTu1Yq33h08",
    "outputId": "77de8205-a351-431f-c55e-5b0866c224b6"
   },
   "outputs": [],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1im_19_v0Fto",
    "outputId": "4a7aa164-be91-4c91-a1b7-668fe8e68818"
   },
   "outputs": [],
   "source": [
    "logits_label = [output['label'] for output in outputs[0]]\n",
    "logits_prob = [(round(output['score'],2)*100) for output in outputs[0]]\n",
    "print(logits_label)\n",
    "print(logits_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-y-fPoRSG19k"
   },
   "outputs": [],
   "source": [
    "# Define a prediction function for LIME\n",
    "def predict_proba(texts):\n",
    "    # Tokenize input texts\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    # Move input tensors to the same device as the model\n",
    "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "# Initialize LIME Explainer\n",
    "explainer = LimeTextExplainer(class_names=[\"Fake\", \"Real\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRdEN6ThNIJb"
   },
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(txt, predict_proba, num_features=int(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "Q-7YchjUNEgi",
    "outputId": "623ea003-a3ae-408f-e047-f0ba28e73bc1"
   },
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mvju41hTCHKE"
   },
   "source": [
    "#For gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4eYRCUiMbvO"
   },
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"Jannat24/finetuned_mbert_fakenews_bn_en_lang\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.eval()\n",
    "\n",
    "# Prediction function for LIME\n",
    "def predict_proba(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "# Function to generate LIME visualization and return as PIL image\n",
    "def generate_lime_visualization(text):\n",
    "    l = len(text.split(\" \"))\n",
    "    pipeline_model = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "    outputs = pipeline_model(text)\n",
    "    logits_prob = [(round(output['score'],2)*100) for output in outputs[0]]\n",
    "    f = str(logits_prob[0])+\" %\"\n",
    "    r = str(logits_prob[1])+\" %\"\n",
    "\n",
    "    # Initialize LIME explainer\n",
    "    explainer = LimeTextExplainer(class_names=[\"Fake\", \"Real\"])\n",
    "    exp = explainer.explain_instance(text, predict_proba, num_features=l)\n",
    "\n",
    "    # Convert explanation to matplotlib figure\n",
    "    fig = exp.as_pyplot_figure()\n",
    "    # Save the figure to a buffer\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plt.close(fig)\n",
    "    # Convert buffer to PIL image\n",
    "    pil_image = Image.open(buf)\n",
    "\n",
    "    #list of words\n",
    "    word_contributions = defaultdict(list)\n",
    "    for word,contribution in exp.as_list():\n",
    "      if contribution <= 0:\n",
    "        word_contributions['Fake'].append((word, contribution))\n",
    "      else:\n",
    "        word_contributions['Real'].append((word, contribution))\n",
    "    fake = [i for i,_ in word_contributions['Fake']]\n",
    "    real = [i for i,_ in word_contributions['Real']]\n",
    "\n",
    "    return f,r,fake,real,pil_image\n",
    "\n",
    "# Define Gradio interface\n",
    "def gradio_interface(text):\n",
    "    return generate_lime_visualization(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQCF4pZuyrAa"
   },
   "outputs": [],
   "source": [
    "gr_interface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=gr.Textbox(lines=5, placeholder=\"Enter text to classify and visualize explanation.\"),\n",
    "    outputs=[ gr.Textbox(label=\"Fake News Probability\"),\n",
    "        gr.Textbox(label=\"Real News Probability\"),\n",
    "        gr.Textbox(label=\"Fake Words\"),\n",
    "        gr.Textbox(label=\"Real Words\"),\n",
    "        gr.Image(type=\"pil\",label=\"Visualization\")],\n",
    "    theme = gr.themes.Soft(),\n",
    "    title=\"Bengali and English Languages Fake-news Identification\",\n",
    "    description=\"Enter text (Bengali or Engish news stories only).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "qjgBCKkS-yw_",
    "outputId": "967f5286-d69a-4802-f1d0-f763078a2480"
   },
   "outputs": [],
   "source": [
    "gr_interface.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfUG0VFG_ocf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
